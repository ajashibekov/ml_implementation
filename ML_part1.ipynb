{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing ML algorithms from scratch\n",
    "## Part 1: Linear Regression, Locally Weighted Linear Regression, Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity, let us start with a simpler example of implementing a linear regression, which we could \"upgrade\" to logistic regression. That would help us understand the fundamentals needed later. *Please note that the bias term is omitted in a lot of models!* <br />\n",
    "<h2>Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a set of features (input variables) and a target output. This would be a single training example. We have a number of training examples, and our purpose is to come up with a function that takes features as an input and outputs something very close to the desired output. <br />\n",
    "\n",
    "*Notation*: <br />\n",
    "For a single training example:  $\\textbf{x}_i \\in \\text{R}^m$ and $y_i \\in \\text{R}$ where $i = 1, ..., n$ <br />\n",
    "$\\textbf{w} \\in \\text{R}^m$ - vector of weights (to be learned)\n",
    "\n",
    "We have a number of $n$ datapoints: $(\\textbf{x}_i, y_i)$. Our purpose is minimize the error function (i.e. make our predictions as close as possible to target outputs): $E(\\textbf{w}) = \\frac{1}{2}\\sum\\limits_{k=1}^n (y_k - \\textbf{w}^T\\textbf{x}_k)^2$. More precisely, we need to come up with weights $\\textbf{w}$ that minimize the given error function (the error function is squared error cost function, half is appended just for the sake of convenience). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do that, we will use a method called gradient descent. You can read more about it here: https://en.wikipedia.org/wiki/Gradient_descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main point of this method is to initialize weights to some random small values, then apply the following rule: <br />\n",
    "$\\textbf{w} \\leftarrow \\textbf{w} - \\alpha \\nabla E(\\textbf{w})$, or <br />\n",
    "$w_i \\leftarrow w_i - \\alpha \\frac{dE}{dw_i}$ , where $\\alpha$ is a so called learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that $E(\\textbf{w}) = \\frac{1}{2}\\sum\\limits_{k=1}^n (y_k - \\textbf{w}^T\\textbf{x}_k)^2$, it is easy to derive $\\frac{dE}{dw_i}$, which is simply $\\sum\\limits_{k=1}^n (y_k - \\textbf{w}^T\\textbf{x}_k) (-x_{k,i})$ <br />\n",
    "Hence the update rule is now simply: <br />\n",
    "$w_i \\leftarrow w_i + \\alpha \\sum\\limits_{k=1}^n (y_k - \\textbf{w}^T\\textbf{x}_k) x_{k,i}$ <br />\n",
    "$\\Delta w_i \\leftarrow \\alpha \\sum\\limits_{k=1}^n (y_k - \\textbf{w}^T\\textbf{x}_k) x_{k,i}$ <br />\n",
    "When do we stop updating? When either a new update is really small in absolute terms, or we have reached the maximum number of iterations. <br />\n",
    "Note that the above update rule uses all the training examples for one single update (and that's why it's called a batch rule) <br />\n",
    "Now let us implement this simple algorithm: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000000e+00  6.09469402e-11 -2.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Suppose we are given N datapoints, then X is a (design) matrix NxM, y is a column vector Nx1. \n",
    "def linear_regression(X, y, max_number_iterations = 10000, learning_rate = 0.01, threshold = 0):\n",
    "    iteration_numb = 0\n",
    "    difference = 1\n",
    "    N = X.shape[0]\n",
    "    M = X.shape[1]\n",
    "    \n",
    "    # initialize w to random small numbers\n",
    "    w = []\n",
    "    for i in range(0, M):\n",
    "        w.append(random.randint(1, 100) / 1000)\n",
    "    w = np.asarray(w)\n",
    "\n",
    "    while iteration_numb < max_number_iterations and difference > threshold:\n",
    "        delta = np.zeros(M)\n",
    "        # for every training example we have:\n",
    "        for k in range(0, N):\n",
    "            # calculate y_k - x_k*w\n",
    "            error = y[k] - np.dot(X[k,:], w)\n",
    "            for i in range(0, M):\n",
    "                delta[i] = delta[i] + learning_rate * X[k, i] * error\n",
    "        for i in range(0, M):\n",
    "            w[i] = w[i] + delta[i];\n",
    "            \n",
    "        iteration_numb += 1\n",
    "        difference = np.sqrt(delta.dot(delta))\n",
    "    return w\n",
    "\n",
    "# Just a simple test\n",
    "# Note that the output y = 1*x1 + 0*x2 - 2*x3, so we expect the weights to be 1, 0 and -2\n",
    "X = np.array([[5, 3, 2], [4, 2, 1], [3, 7, 0]])\n",
    "y = np.array([1, 2, 3])\n",
    "w = linear_regression(X, y)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that we reached a correct set of weights by finding the global minimum weights using normal equations. <br />\n",
    "For more details please refer to https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)#Derivation_directly_in_terms_of_matrices <br />\n",
    "In general, the solution is given by $\\textbf{w} = (X^TX)^{-1}X^Ty$, where $X$ is a design matrix (i.e. the matrix, $i$-th row of which is the $i$-th training example, $\\textbf{x}_i$) <br />\n",
    "Note that sometimes it is not possible to derive an analytical solution (the case when the inverse does not exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.0000000e+00 -8.8817842e-16 -2.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import inv\n",
    "# Check with analytical solution using normal equations\n",
    "analytical_w = inv(X.transpose() @ X) @ X.transpose() @ y\n",
    "print(analytical_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that our linear regression model is working well (both solutions are identical) <br />\n",
    "Now that we have implemented a simple batch algorith, let us include extension for stochastic and mini-batch training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000000e+00  5.93681512e-11 -2.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "def lin_regression_advanced(X, y, batch_size = None, max_numb_iterations = 10000, learning_rate = 0.01, threshold = 0):\n",
    "    iteration_numb = 0\n",
    "    difference = 1\n",
    "    N = X.shape[0]\n",
    "    M = X.shape[1]\n",
    "    if batch_size == None:\n",
    "        # Default to batch training\n",
    "        batch_size = N\n",
    "    \n",
    "    full_data = np.column_stack([X, y])\n",
    "    \n",
    "    # initialize w to random small numbers\n",
    "    w = []\n",
    "    for i in range(0, M):\n",
    "        w.append(random.randint(1, 100) / 1000)\n",
    "    w = np.asarray(w)\n",
    "\n",
    "    while iteration_numb < max_numb_iterations and difference > threshold:\n",
    "        np.random.shuffle(full_data)\n",
    "        \n",
    "        mini_batches = [\n",
    "            full_data[k : k + batch_size]\n",
    "            for k in range(0, N, batch_size)]\n",
    "        \n",
    "        for mini_batch in mini_batches:\n",
    "            delta = np.zeros(M)\n",
    "            \n",
    "            for training_example in mini_batch:\n",
    "                # training example is of shape (M+1), with M features (x) and 1 output (y)\n",
    "                error = training_example[-1] - np.dot(training_example[:-1], w)\n",
    "                for i in range(0, M):\n",
    "                    delta[i] = delta[i] + learning_rate * training_example[i] * error\n",
    "            \n",
    "            #Once this mini-batch is over, update weights:\n",
    "            for i in range(0, M):\n",
    "                w[i] = w[i] + delta[i]\n",
    "                \n",
    "            # Optional: if delta change is really small (smaller than threshold), break from this loop,\n",
    "            # return the current weights\n",
    "            difference = np.sqrt(delta.dot(delta))\n",
    "            if(difference < threshold):\n",
    "                break\n",
    "            \n",
    "            \n",
    "        iteration_numb += 1\n",
    "    return w\n",
    "\n",
    "# Just a simple test\n",
    "X = np.array([[5, 3, 2], [4, 2, 1], [3, 7, 0]])\n",
    "y = np.array([1, 2, 3])\n",
    "w = lin_regression_advanced(X, y, batch_size = 10)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the simplest ML model is implemented, let us move on to a slightly more complicated one. \n",
    "<h2> Locally Weighted Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is slightly more complicated, as compared to the ordinary linear regression, we weight each error. You could read more about it on the web. <br />\n",
    "In mathematical terms, our objective now is to minimize the following cost function (I still denote the weights that should be *learned* as $\\textbf{w}$, whereas the weights that are parameters of this model would be denoted as $a_i$ to avoid confusion): <br />\n",
    "$E(\\textbf{w}) = \\frac{1}{2}\\sum\\limits_{k=1}^n a_k(y_k - \\textbf{w}^T\\textbf{x}_k)^2$ <br /> A fairly standard choice for $a_k$ is $e^{-\\frac{(\\textbf{x}_k - \\textbf{x})^2}{2\\tau^2}}$ where $\\tau$ is a bandwidth parameter. <br />\n",
    "The update rule is now simply: <br />\n",
    "$w_i \\leftarrow w_i + \\alpha \\sum\\limits_{k=1}^n a_k(y_k - \\textbf{w}^T\\textbf{x}_k) x_{k,i}$ <br />\n",
    "$\\Delta w_i \\leftarrow \\alpha \\sum\\limits_{k=1}^n a_k(y_k - \\textbf{w}^T\\textbf{x}_k) x_{k,i}$ <br />\n",
    "Note that the algorithm is non-parametric, so in order to predict an output for a new input, we train a new model. <br />\n",
    "For normal equations part: <br />\n",
    "$\\textbf{w} = (X^TAX)^{-1}X^TAy$, where A is a diagonal matrix containing the weights $a_k$ on the main diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9999999999999627\n",
      "[ 1.00000000e+00  2.03173538e-14 -2.00000000e+00]\n",
      "2.000000000000006\n",
      "[ 1.00000000e+00  3.44169138e-15 -2.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# Non-parametric regression, so we predict a value for a given input right away\n",
    "def loc_weighted_regression(training_X, y, input_X, bandwidth = 0.5, max_number_iterations = 50000, \n",
    "                            learning_rate = 0.01, threshold = 0):\n",
    "    iteration_numb = 0\n",
    "    difference = 1\n",
    "    N = training_X.shape[0]\n",
    "    M = training_X.shape[1]\n",
    "    \n",
    "    # initialize w to random small numbers\n",
    "    w = []\n",
    "    for i in range(0, M):\n",
    "        w.append(random.randint(1, 100) / 1000)\n",
    "    w = np.asarray(w)\n",
    "    \n",
    "    # initialize a\n",
    "    a = np.zeros(N)\n",
    "    for i in range(0, N):\n",
    "        intermed_value = np.dot(training_X[i, :] - input_X, training_X[i, :] - input_X)\n",
    "        a[i] = -intermed_value / (2* bandwidth ** 2)\n",
    "    a = np.exp(a)\n",
    "\n",
    "    while iteration_numb < max_number_iterations and difference > threshold:\n",
    "        delta = np.zeros(M)\n",
    "        # for every training example we have:\n",
    "        for k in range(0, N):\n",
    "            # calculate y_k - x_k*w\n",
    "            error = y[k] - np.dot(training_X[k,:], w)\n",
    "            for i in range(0, M):\n",
    "                delta[i] = delta[i] + a[k] * learning_rate * training_X[k, i] * error\n",
    "        for i in range(0, M):\n",
    "            w[i] = w[i] + delta[i];\n",
    "            \n",
    "        iteration_numb += 1\n",
    "        difference = np.sqrt(delta.dot(delta))\n",
    "    # We learned the weights, so just output the prediction\n",
    "    return (w, np.dot(w, input_X))\n",
    "\n",
    "# Obtain an analytical solution using normal equations\n",
    "def loc_weighted_normal_eq(training_X, y, input_X, bandwidth = 0.5):\n",
    "    # initialize a\n",
    "    N = X.shape[0]\n",
    "    a = np.zeros(N)\n",
    "    for i in range(0, N):\n",
    "        intermed_value = np.dot(training_X[i, :] - input_X, training_X[i, :] - input_X)\n",
    "        a[i] = -intermed_value / (2* bandwidth ** 2)\n",
    "    a = np.exp(a)\n",
    "    a = np.diag(a)\n",
    "    w = inv(training_X.transpose() @ a @ X) @ training_X.transpose() @ a @ y\n",
    "    return (w, np.dot(w, input_X))\n",
    "\n",
    "# Just a simple test\n",
    "X = np.array([[5, 3, 2], [4, 2, 1], [3, 7, 0]])\n",
    "y = np.array([1, 2, 3])\n",
    "input_X = np.array([8, 2, 3])\n",
    "\n",
    "(w, pred) = loc_weighted_regression(X, y, input_X, bandwidth = 3)\n",
    "(analytical_w, analytical_pred) = loc_weighted_normal_eq(X, y, input_X, bandwidth = 3)\n",
    "print(pred)\n",
    "print(w)\n",
    "print(analytical_pred)\n",
    "print(analytical_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the importance of the bandwidth parameter. The higher the bandwidth, the more points we look at (around the given one). Note that the algorithm would fail to achieve a proper result, if the bandwidth is too low (in our example, *input_X* is quite far from training points). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21300608492972706\n",
      "[0.01800059 0.01500035 0.01300023]\n",
      "-2.9912676598669776\n",
      "[ 0.31449722  2.15795708 -3.27438652]\n"
     ]
    }
   ],
   "source": [
    "(w, pred) = loc_weighted_regression(X, y, input_X, bandwidth = .5)\n",
    "(analytical_w, analytical_pred) = loc_weighted_normal_eq(X, y, input_X, bandwidth = .5)\n",
    "print(pred)\n",
    "print(w)\n",
    "print(analytical_pred)\n",
    "print(analytical_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Logistic Regression </h2>\n",
    "Before, we were using ML models to predict a continuous variable. What if, instead, you need to classify datapoints according to just a few categories (e.g. input: picture -> output: like/dislike). This problem is called a *classification* problem, and one of the easiest models used for classification is logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a binary classification (i.e. there are only two possible outcomes: 0 and 1). <br />\n",
    "Instead of simply using $\\textbf{w}^T\\textbf{x}_i$ as an output of the model, we will use a so-called logistic (sigmoid) function of $\\textbf{w}^T\\textbf{x}_i$, which would be $\\frac{1}{1 + e^{-\\textbf{w}^T\\textbf{x}_i}}$ <br />\n",
    "The function that we aim to maximize (why is it a maximization problem now? This function is derived from probabilistic interpretaion \\[ maximum likelihood \\]. For those who are interested, please refer to CS229 Stanford Machine Learning course notes) is $\\sum\\limits_{i=1}^m{y_i \\log (\\textbf{w}^T\\textbf{x}_i) + (1 - y_i)\\log(1 - \\textbf{w}^T\\textbf{x}_i)}$ <br />\n",
    "It is easy to derive the gradient ascent (note: we are using gradient ascent now, as we are maximizing the function) rule, which is: <br />\n",
    "$w_i \\leftarrow w_i + \\alpha \\sum\\limits_{k=1}^n (y_k - sigm(\\textbf{w}^T\\textbf{x}_k)) x_{k,i}$ <br />\n",
    "$\\Delta w_i \\leftarrow \\alpha \\sum\\limits_{k=1}^n (y_k - sigm(\\textbf{w}^T\\textbf{x}_k)) x_{k,i}$ (for batch training) <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_regression(X, y, batch_size = None, max_numb_iterations = 10000, learning_rate = 0.1, threshold = 0):\n",
    "    iteration_numb = 0\n",
    "    difference = 1\n",
    "    N = X.shape[0]\n",
    "    M = X.shape[1]\n",
    "    if batch_size == None:\n",
    "        # Default to batch training\n",
    "        batch_size = N\n",
    "    \n",
    "    full_data = np.column_stack([X, y])\n",
    "    \n",
    "    # initialize w to random small numbers\n",
    "    w = []\n",
    "    for i in range(0, M):\n",
    "        w.append(random.randint(1, 100) / 1000)\n",
    "    w = np.asarray(w)\n",
    "\n",
    "    while iteration_numb < max_numb_iterations and difference > threshold:\n",
    "        np.random.shuffle(full_data)\n",
    "        \n",
    "        mini_batches = [\n",
    "            full_data[k : k + batch_size]\n",
    "            for k in range(0, N, batch_size)]\n",
    "        \n",
    "        for mini_batch in mini_batches:\n",
    "            delta = np.zeros(M)\n",
    "            \n",
    "            for training_example in mini_batch:\n",
    "                # training example is of shape (M+1), with M features (x) and 1 output (y)\n",
    "                error = training_example[-1] - sigmoid(np.dot(training_example[:-1], w))\n",
    "                for i in range(0, M):\n",
    "                    delta[i] = delta[i] + learning_rate * training_example[i] * error\n",
    "            \n",
    "            #Once this mini-batch is over, update weights:\n",
    "            for i in range(0, M):\n",
    "                w[i] = w[i] + delta[i]\n",
    "                \n",
    "            # Optional: if delta change is really small (smaller than threshold), break from this loop,\n",
    "            # return the current weights\n",
    "            difference = np.sqrt(delta.dot(delta))\n",
    "            if(difference < threshold):\n",
    "                break\n",
    "            \n",
    "        iteration_numb += 1\n",
    "    return w\n",
    "\n",
    "def predict(input_X, w):\n",
    "    res = np.dot(input_X, w)\n",
    "    if res >= .5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know if our simple logistic regression works satisfactorily? Let us check with the sklearn LogisticRegression model. Please note that sklearn's LogisticRegression automatically regularizes (we will cover regularization a bit later) and fits intercept point (or *bias term*, which I omitted for the purpose of simplicity). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.00158627 3.07228656]\n",
      "1\n",
      "[1]\n",
      "[[3.03910611 3.11125592]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[-1, -2], [-2, -1], [6, 7], [2, 3]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "w = logistic_regression(X, y, batch_size = 4)\n",
    "pred = predict(np.array([1.2, -1]), w)\n",
    "print(w)\n",
    "print(pred)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Set regularization parameter to a large number so that there is almost no regularization (to compare with our \n",
    "# unregularized model)\n",
    "lr = LogisticRegression(fit_intercept = False, C = 1e22)\n",
    "lr.fit(X, y)\n",
    "print(lr.predict(np.array([1.2, -1]).reshape(1, -1)))\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the results are quite similar (learned weights differ only slightly, due to small number of iterations). <br />\n",
    "Now let us introduce the concept of regularization into the Logistic Regression model. ( A good read on regularization: https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will solidify the logistic regression model by wrapping it into a Python class. We will keep only Stochastic Gradient Descent for the purpose of simplicity and we will also introduce the concept of vectorization as well (i.e. we will get rid of unnecessary for-loops and use numpy's ability to efficiently operate on arrays and matrices). For the illustration purposes, we will introduce a bias term (intercept term) in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-14.09050177  -5.05837918   8.28854616]\n"
     ]
    }
   ],
   "source": [
    "class MyLogisticRegression:\n",
    "    \n",
    "    '''\n",
    "    fit_intercept - whether the bias term is included or not\n",
    "    penalty - whether and what regularization to use (possible values: 'l1', 'l2' or 'none')\n",
    "    C - regularization parameter\n",
    "    threshold - tolerance level for stopping\n",
    "    max_iter - maximum number of iterations before stopping\n",
    "    lr - learning rate for SGD\n",
    "    '''\n",
    "    def __init__(self, fit_intercept = True, penalty = 'none', C = 1.0, threshold = 0, max_iter = 10000, lr = 0.1):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.penalty = penalty\n",
    "        self.C = C\n",
    "        self.threshold = threshold\n",
    "        self.max_iter = max_iter\n",
    "        self.lr = lr\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        iteration_numb = 0\n",
    "        difference = 1\n",
    "        N = X.shape[0]\n",
    "        M = X.shape[1]\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            M += 1\n",
    "            X = np.hstack([np.ones((N, 1)), X])\n",
    "            \n",
    "        # initialize w to random small numbers\n",
    "        self.w = []\n",
    "        for i in range(0, M):\n",
    "            self.w.append(random.randint(1, 100) / 1000)\n",
    "        self.w = np.asarray(self.w)\n",
    "            \n",
    "        while iteration_numb < self.max_iter and difference > self.threshold:\n",
    "            # Note that we got rid of all the loops. That is vectorization\n",
    "            errors = y - 1 / (1 + np.exp(- X @ self.w))\n",
    "            \n",
    "            # Add in regularization.\n",
    "            if self.penalty == 'l2':\n",
    "                add_on =  - (1 / self.C) * self.w\n",
    "            elif self.penalty == 'l1':\n",
    "                add_on = - (1 / self.C) * np.sign(self.w)\n",
    "            else:\n",
    "                add_on = np.zeros(M)\n",
    "                \n",
    "            '''\n",
    "            Note that sklearn algorithms regularize the intercept term, even though\n",
    "            it is more common to exclude regularization for the intercept term\n",
    "            \n",
    "            In case we do not want to regularize the intercept term, use the following code snippet:\n",
    "            if self.fit_intercept:\n",
    "                add_on[0] = 0 # no regularization add-on for the intercept term\n",
    "            ''' \n",
    "            delta = self.lr * (X.transpose() @ errors + add_on)\n",
    "            self.w += delta\n",
    "            \n",
    "            # Optional: if delta change is really small (smaller than threshold), break from this loop,\n",
    "            # return the current weights\n",
    "            difference = np.sqrt(delta.dot(delta))\n",
    "            if(difference < self.threshold):\n",
    "                break\n",
    "                \n",
    "            iteration_numb += 1\n",
    "                \n",
    "    def predict(self, input_X):\n",
    "        result = np.dot(input_X, self.w)\n",
    "        if result >= .5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def coef_(self):\n",
    "        return self.w\n",
    "        \n",
    "# Create a bigger random sample dataset\n",
    "np.random.seed(12)\n",
    "num_observations = 5000\n",
    "x1 = np.random.multivariate_normal([0, 0], [[1, .75],[.75, 1]], num_observations)\n",
    "x2 = np.random.multivariate_normal([1, 4], [[1, .75],[.75, 1]], num_observations)\n",
    "\n",
    "X = np.vstack((x1, x2)).astype(np.float32)\n",
    "y = np.hstack((np.zeros(num_observations),\n",
    "                              np.ones(num_observations)))\n",
    "\n",
    "lr = MyLogisticRegression(fit_intercept = True, lr = 5e-4, max_iter = 20000, threshold = 1e-10, penalty = 'l1', C = 3e15)\n",
    "\n",
    "lr.fit(X, y)\n",
    "w = lr.coef_()\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.05901197  8.28958301]]\n",
      "[-14.09229942]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Set regularization parameter to a large number so that there is almost no regularization (to compare with our \n",
    "# unregularized model)\n",
    "lr = LogisticRegression(fit_intercept = True, C = 3e15, max_iter = 1000, tol = 1e-10, penalty = 'l1')\n",
    "lr.fit(X, y)\n",
    "print(lr.coef_)\n",
    "print(lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our logistic regression model works good enough. <br />\n",
    "In this notebook, we've introduced a lot of simple models (Linear Regression, Locally Weighted Linear Regression and Logistic Regression). We also introduced a number of important concepts/algorithms (Gradient Descent: Batch, Stochastic and Mini-Batch; Bias/Intercept Term; L1/L2 regularization; Vectorization). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
